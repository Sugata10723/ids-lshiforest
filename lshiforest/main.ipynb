{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf63d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.7 (main, Aug 14 2025, 11:12:11) [Clang 17.0.0 (clang-1700.0.13.3)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from src.detectors import LSHiForest\n",
    "import data_loader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affa90d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- index\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# dataのロード:unsw\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_unsw_nb15\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test))\n",
      "File \u001b[0;32m~/ids-lshiforest/lshiforest/data_loader.py:76\u001b[0m, in \u001b[0;36mload_unsw_nb15\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 76\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, y_train, X_test, y_test\n",
      "File \u001b[0;32m~/ids-lshiforest/lshiforest/data_loader.py:110\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(X_train, X_test, n)\u001b[0m\n\u001b[1;32m    108\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m    109\u001b[0m X_train_sc \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m--> 110\u001b[0m X_test_sc \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# PCA\u001b[39;00m\n\u001b[1;32m    113\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m~/ids-lshiforest/lshiforest/.venv/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/ids-lshiforest/lshiforest/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:1075\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1072\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1074\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1075\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/ids-lshiforest/lshiforest/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2929\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[1;32m   2846\u001b[0m     _estimator,\n\u001b[1;32m   2847\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2853\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m   2854\u001b[0m ):\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[1;32m   2856\u001b[0m \n\u001b[1;32m   2857\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2929\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2930\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[1;32m   2931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[0;32m~/ids-lshiforest/lshiforest/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2787\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m   2785\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2787\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- index\n"
     ]
    }
   ],
   "source": [
    "# dataのロード:cic\n",
    "X_train, y_train, X_test, y_test, y_cat = dl.load_cic_ids()\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先行研究で実験\n",
    "num_ensemblers = 100\n",
    "classifiers = [(\"sklearn.ISO\", IsolationForest(n_estimators=num_ensemblers)), (\"ALSH\", LSHiForest('ALSH', num_ensemblers)), (\"L2SH\", LSHiForest('L2SH', num_ensemblers)), (\"L1SH\", LSHiForest('L1SH', num_ensemblers)) ]\n",
    "\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(classifiers):\n",
    "\t\n",
    "\tprint(\"\\n\"+clf_name+\":\")\n",
    "\tstart_time = time.time()\n",
    "\tclf.fit(X_train)\n",
    "\ttrain_time = time.time()-start_time\n",
    "\ty_pred = clf.decision_function(X_test)\n",
    "\ttest_time = time.time()-start_time-train_time\n",
    "\tauc = roc_auc_score(y_test, y_pred)\n",
    "\t\n",
    "\tprint(\"\\tAUC score:\\t\", auc)\n",
    "\tprint(\"\\tTraining time:\\t\", train_time) \n",
    "\tprint(\"\\tTesting time:\\t\", test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataのロード:unsw\n",
    "X_train, y_train, X_test, y_test = dl.load_unsw_nb15()\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d151711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先行研究で実験\n",
    "num_ensemblers = 100\n",
    "classifiers = [(\"sklearn.ISO\", IsolationForest(n_estimators=num_ensemblers)), (\"ALSH\", LSHiForest('ALSH', num_ensemblers)), (\"L2SH\", LSHiForest('L2SH', num_ensemblers)), (\"L1SH\", LSHiForest('L1SH', num_ensemblers)) ]\n",
    "\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(classifiers):\n",
    "\t\n",
    "\tprint(\"\\n\"+clf_name+\":\")\n",
    "\tstart_time = time.time()\n",
    "\tclf.fit(X_train)\n",
    "\ttrain_time = time.time()-start_time\n",
    "\ty_pred = clf.decision_function(X_test)\n",
    "\ttest_time = time.time()-start_time-train_time\n",
    "\tauc = roc_auc_score(y_test, y_pred)\n",
    "\t\n",
    "\tprint(\"\\tAUC score:\\t\", auc)\n",
    "\tprint(\"\\tTraining time:\\t\", train_time) \n",
    "\tprint(\"\\tTesting time:\\t\", test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07151d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ensemblers = 100\n",
    "classifiers = [\n",
    "    (\"sklearn.ISO\", IsolationForest(n_estimators=num_ensemblers)), \n",
    "    (\"ALSH\", LSHiForest('ALSH', num_ensemblers)), \n",
    "    (\"L2SH\", LSHiForest('L2SH', num_ensemblers)), \n",
    "    (\"L1SH\", LSHiForest('L1SH', num_ensemblers))\n",
    "]\n",
    "\n",
    "attack_cat_series = pd.read_csv('data/unsw_nb15/UNSW_NB15_training-set.csv')['attack_cat']\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'attack_cat': attack_cat_series.values,\n",
    "    'label': y_test.values\n",
    "}, index=y_test.index)\n",
    "\n",
    "\n",
    "# --- 3. モデルの学習・予測・結果の保存 ---\n",
    "if results_df is not None:\n",
    "    # X_testは数値データのみなので、そのまま予測に使用。\n",
    "    for clf_name, clf in classifiers:\n",
    "        print(f\"\\nTraining and Predicting with {clf_name}...\")\n",
    "        clf.fit(X_train)\n",
    "        y_pred = clf.decision_function(X_test)\n",
    "        results_df[f'pred_{clf_name}'] = y_pred\n",
    "\n",
    "    normal_df = results_df[results_df['label'] == 0]\n",
    "    attack_df = results_df[results_df['label'] == 1]\n",
    "    \n",
    "    corrected_auc_scores = {}\n",
    "    for clf_name, _ in classifiers:\n",
    "        scores = {}\n",
    "        unique_attack_cats = sorted(attack_df['attack_cat'].unique())\n",
    "        for cat in unique_attack_cats:\n",
    "            current_attack_df = attack_df[attack_df['attack_cat'] == cat]\n",
    "            eval_df = pd.concat([normal_df, current_attack_df])\n",
    "            eval_preds = eval_df[f'pred_{clf_name}']\n",
    "            eval_labels = eval_df['label']\n",
    "            auc = roc_auc_score(eval_labels, eval_preds)\n",
    "            scores[cat] = auc\n",
    "        corrected_auc_scores[clf_name] = scores\n",
    "\n",
    "    # グラフ描画と結果表示\n",
    "    plot_df = pd.DataFrame(corrected_auc_scores).T.rename_axis('Classifier').reset_index()\n",
    "    plot_df_melted = plot_df.melt(id_vars='Classifier', var_name='Attack Category', value_name='AUC Score').dropna()\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(18, 10))\n",
    "    sns.barplot(data=plot_df_melted, x='Attack Category', y='AUC Score', hue='Classifier', ax=ax)\n",
    "    ax.set_title('AUC Score by Attack Category (vs. Normal)', fontsize=20)\n",
    "    ax.set_xlabel('Attack Category', fontsize=14)\n",
    "    ax.set_ylabel('AUC Score', fontsize=14)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.tick_params(axis='x', labelrotation=45, labelsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.legend(title='Classifier', fontsize=12)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Corrected AUC Scores (Each Attack vs. Normal):\")\n",
    "    pd.options.display.float_format = '{:.4f}'.format\n",
    "    results_table = pd.DataFrame(corrected_auc_scores)\n",
    "    print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d151e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備:nsl\n",
    "X_train, y_train, X_test, y_test = dl.load_nsl_kdd()\n",
    "\n",
    "print(f\"Train data size :{len(X_train)}\")\n",
    "print(f\"Test data size :{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ensemblers = 100\n",
    "classifiers = [(\"sklearn.ISO\", IsolationForest(n_estimators=num_ensemblers)), (\"ALSH\", LSHiForest('ALSH', num_ensemblers)), (\"L2SH\", LSHiForest('L2SH', num_ensemblers)), (\"L1SH\", LSHiForest('L1SH', num_ensemblers)) ]\n",
    "for i, (clf_name, clf) in enumerate(classifiers):\n",
    "\t\n",
    "\tprint(\"\\n\"+clf_name+\":\")\n",
    "\tstart_time = time.time()\n",
    "\t\n",
    "\tclf.fit(X_train)\n",
    "\t\n",
    "\ttrain_time = time.time()-start_time\n",
    "\n",
    "\ty_pred = clf.decision_function(X_test)\n",
    "\n",
    "\tif clf_name == \"sklearn.ISO\":\n",
    "\t\ty_pred = -y_pred\n",
    "\n",
    "\t\n",
    "\ttest_time = time.time()-start_time-train_time\n",
    "\t\n",
    "\tauc = roc_auc_score(y_test, y_pred)\n",
    "\t\n",
    "\tprint(\"\\tAUC score:\\t\", auc)\n",
    "\tprint(\"\\tTraining time:\\t\", train_time) \n",
    "\tprint(\"\\tTesting time:\\t\", test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ensemblers = 100\n",
    "classifiers = [\n",
    "    (\"sklearn.ISO\", IsolationForest(n_estimators=num_ensemblers)),\n",
    "    (\"ALSH\", LSHiForest('ALSH', num_ensemblers)),\n",
    "    (\"L2SH\", LSHiForest('L2SH', num_ensemblers)),\n",
    "    (\"L1SH\", LSHiForest('L1SH', num_ensemblers))\n",
    "]\n",
    "\n",
    "# --- 2.【重要】NSL-KDDの攻撃カテゴリ名を取得し、予測結果を格納するDataFrameを準備 ---\n",
    "results_df_created = False\n",
    "try:\n",
    "    test_df_for_cat = pd.read_csv('data/nsl-kdd/KDDTest+.txt', header=None)\n",
    "    \n",
    "    # カラム名はdata_loader.pyの定義に合わせます。\n",
    "    columns = [\n",
    "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "        'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "        'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "        'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n",
    "        'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
    "        'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "        'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "        'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "        'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack',\n",
    "        'difficulty'\n",
    "    ]\n",
    "    test_df_for_cat.columns = columns\n",
    "    # attack列の値を空白除去\n",
    "    attack_cat_series = test_df_for_cat['attack'].str.strip()\n",
    "    results_df_created = True\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data/nsl_kdd/KDDTest+.txt' が見つかりません。ファイルパスを確認してください。\")\n",
    "\n",
    "\n",
    "if results_df_created:\n",
    "    # y_test（0/1ラベル）とattack_cat（攻撃カテゴリ名）でDataFrameを作成\n",
    "    results_df = pd.DataFrame({\n",
    "        'attack_cat': attack_cat_series.values,\n",
    "        'label': y_test.values\n",
    "    }, index=y_test.index)\n",
    "\n",
    "    # --- 3. モデルの学習・予測・結果の保存 ---\n",
    "    for clf_name, clf in classifiers:\n",
    "        print(f\"\\nTraining and Predicting with {clf_name}...\")\n",
    "        clf.fit(X_train)\n",
    "        y_pred = clf.decision_function(X_test)\n",
    "        results_df[f'pred_{clf_name}'] = y_pred\n",
    "    print(\"\\n--- All predictions are stored in results_df ---\")\n",
    "\n",
    "    # --- 4. カテゴリ別AUCの計算 ---\n",
    "    normal_df = results_df[results_df['label'] == 0]\n",
    "    attack_df = results_df[results_df['label'] == 1]\n",
    "\n",
    "    corrected_auc_scores = {}\n",
    "    for clf_name, _ in classifiers:\n",
    "        scores = {}\n",
    "        unique_attack_cats = sorted([cat for cat in attack_df['attack_cat'].unique() if cat != 'normal'])\n",
    "        for cat in unique_attack_cats:\n",
    "            current_attack_df = attack_df[attack_df['attack_cat'] == cat]\n",
    "            eval_df = pd.concat([normal_df, current_attack_df])\n",
    "            eval_preds = eval_df[f'pred_{clf_name}']\n",
    "            eval_labels = eval_df['label']\n",
    "            \n",
    "            if len(eval_labels.unique()) > 1:\n",
    "                auc = roc_auc_score(eval_labels, -eval_preds)\n",
    "                scores[cat] = auc\n",
    "            else:\n",
    "                scores[cat] = None\n",
    "        corrected_auc_scores[clf_name] = scores\n",
    "\n",
    "    # --- 5. グラフ描画と結果表示 ---\n",
    "    plot_df = pd.DataFrame(corrected_auc_scores).T.rename_axis('Classifier').reset_index()\n",
    "    plot_df_melted = plot_df.melt(id_vars='Classifier', var_name='Attack Category', value_name='AUC Score').dropna()\n",
    "\n",
    "    # カテゴリごとの平均AUCを算出し、降順でカテゴリの順序を決定\n",
    "    category_order = plot_df_melted.groupby('Attack Category')['AUC Score'].mean().sort_values(ascending=False).index\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(18, 10))\n",
    "    # order引数に決定した順序を指定\n",
    "    sns.barplot(data=plot_df_melted, x='Attack Category', y='AUC Score', hue='Classifier', ax=ax, order=category_order)\n",
    "    \n",
    "    ax.set_title('NSL-KDD: AUC Score by Attack Category (vs. Normal)', fontsize=20)\n",
    "    ax.set_xlabel('Attack Category', fontsize=14)\n",
    "    ax.set_ylabel('AUC Score', fontsize=14)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.tick_params(axis='x', labelrotation=90, labelsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.legend(title='Classifier', fontsize=12)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nsl_kdd_auc_by_category.png')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nCorrected AUC Scores for NSL-KDD (Each Attack vs. Normal):\")\n",
    "    pd.options.display.float_format = '{:.4f}'.format\n",
    "    # テーブルもグラフの順序に合わせて表示\n",
    "    results_table = pd.DataFrame(corrected_auc_scores).reindex(category_order, axis=1) \n",
    "    print(results_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
